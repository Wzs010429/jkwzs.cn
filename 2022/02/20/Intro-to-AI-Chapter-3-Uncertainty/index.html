<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>Intro to AI Chapter 3 Uncertainty | 南花醉笔丶の个人博客</title><meta name="keywords" content="Artificial Intelligence,笔记"><meta name="author" content="南花醉笔,jkwzs0429@gmail.com"><meta name="copyright" content="南花醉笔"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="【哈佛大学】CS50.3 人工智能导论·2020 Chapter·3·Uncertainty">
<meta property="og:type" content="article">
<meta property="og:title" content="Intro to AI Chapter 3 Uncertainty">
<meta property="og:url" content="https://jkwzs.cn/2022/02/20/Intro-to-AI-Chapter-3-Uncertainty/index.html">
<meta property="og:site_name" content="南花醉笔丶の个人博客">
<meta property="og:description" content="【哈佛大学】CS50.3 人工智能导论·2020 Chapter·3·Uncertainty">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://jkwzs.cn/img/img_4.png">
<meta property="article:published_time" content="2022-02-19T17:49:38.000Z">
<meta property="article:modified_time" content="2022-02-22T12:11:22.568Z">
<meta property="article:author" content="南花醉笔">
<meta property="article:tag" content="Artificial Intelligence">
<meta property="article:tag" content="笔记">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://jkwzs.cn/img/img_4.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://jkwzs.cn/2022/02/20/Intro-to-AI-Chapter-3-Uncertainty/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><meta name="baidu-site-verification" content="code-q34fyLJTbp"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"We didn't find any results for the search: ${query}"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: {"limitCount":50,"languages":{"author":"Author: 南花醉笔","link":"Link: ","source":"Source: 南花醉笔丶の个人博客","info":"Copyright is owned by the author. For commercial reprints, please contact the author for authorization. For non-commercial reprints, please indicate the source."}},
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"Traditional Chinese Activated Manually","cht_to_chs":"Simplified Chinese Activated Manually","day_to_night":"Dark Mode Activated Manually","night_to_day":"Light Mode Activated Manually","bgLight":"#49b1f5","bgDark":"#1f1f1f","position":"top-right"},
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Intro to AI Chapter 3 Uncertainty',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2022-02-23 01:11:22'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/custom.css"  media="defer" onload="this.media='all'"><!-- hexo injector head_end start --><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Zfour/hexo-electric-clock@1.0.6/clock.css"><!-- hexo injector head_end end --><meta name="generator" content="Hexo 5.4.1"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/myself.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data is-center"><div class="data-item"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">12</div></a></div><div class="data-item"><a href="/tags/"><div class="headline">Tags</div><div class="length-num">11</div></a></div><div class="data-item"><a href="/categories/"><div class="headline">Categories</div><div class="length-num">7</div></a></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);" rel="external nofollow noreferrer"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" target="_blank" rel="noopener external nofollow noreferrer" href="https://music.163.com/#/user/home?id=122634067"><i class="fa-fw fas fa-music"></i><span> Cloud Music</span></a></li><li><a class="site-page child" target="_blank" rel="noopener external nofollow noreferrer" href="https://space.bilibili.com/22035617"><i class="fa-fw fas fa-video"></i><span> Bilibili</span></a></li><li><a class="site-page child" href="/bangumis/"><i class="fa-fw fa fa-bell"></i><span> 追番</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/img/img_4.png')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">南花醉笔丶の个人博客</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> Search</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);" rel="external nofollow noreferrer"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" target="_blank" rel="noopener external nofollow noreferrer" href="https://music.163.com/#/user/home?id=122634067"><i class="fa-fw fas fa-music"></i><span> Cloud Music</span></a></li><li><a class="site-page child" target="_blank" rel="noopener external nofollow noreferrer" href="https://space.bilibili.com/22035617"><i class="fa-fw fas fa-video"></i><span> Bilibili</span></a></li><li><a class="site-page child" href="/bangumis/"><i class="fa-fw fa fa-bell"></i><span> 追番</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Intro to AI Chapter 3 Uncertainty</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2022-02-19T17:49:38.000Z" title="Created 2022-02-20 06:49:38">2022-02-20</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2022-02-22T12:11:22.568Z" title="Updated 2022-02-23 01:11:22">2022-02-23</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Artificial-Intelligence/">Artificial Intelligence</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">Word count:</span><span class="word-count">4.1k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">Reading time:</span><span>24min</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Intro to AI Chapter 3 Uncertainty"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post View:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="Chapter-3-Uncertainty"><a href="#Chapter-3-Uncertainty" class="headerlink" title="Chapter 3 Uncertainty"></a>Chapter 3 Uncertainty</h1><h2 id="Probability"><a href="#Probability" class="headerlink" title="Probability"></a>Probability</h2><p>Uncertainty can be represented as a number of events and the likelihood, or probability, of each of them happening.</p>
<h3 id="Axioms-in-Probability"><a href="#Axioms-in-Probability" class="headerlink" title="Axioms in Probability"></a>Axioms in Probability</h3><ul>
<li>0 &lt; $P(ω)$ &lt; 1: every value representing probability must range between 0 and 1.<ul>
<li>Zero is an impossible event, like rolling a standard die and getting a 7.</li>
<li>One is an event that is certain to happen, like rolling a standard die and getting a value less than 10.</li>
<li>In general, the higher the value, the more likely the event is to happen.</li>
</ul>
</li>
<li>The probabilities of every possible event, when summed together, are equal to 1.</li>
</ul>
<p>$$<br>\sum_{\omega \in \Omega} P(\omega) &#x3D; 1<br>$$</p>
<p>To get the probability of an event, we divide the number of worlds in which it occurs by the number of total possible worlds. For example, there are 36 possible worlds when rolling two dice. Only in one of these worlds, when both dice yield a 6, do we get the sum of 12. Thus, $P(12) &#x3D; 1&#x2F;36$, or, in words, the probability of rolling two dice and getting two numbers whose sum is 12 is 1&#x2F;36. What is $P(7)$? We count and see that the sum 7 occurs in 6 worlds. Thus, $P(7) &#x3D; 6&#x2F;36 &#x3D; 1&#x2F;6$.</p>
<h3 id="Unconditional-Probability"><a href="#Unconditional-Probability" class="headerlink" title="Unconditional Probability"></a>Unconditional Probability</h3><p>Unconditional probability is the degree of belief in a proposition in the absence of any other evidence. All the questions that we have asked so far were questions of unconditional probability, because the result of rolling a die is not dependent on previous events.</p>
<h2 id="Conditional-Probability"><a href="#Conditional-Probability" class="headerlink" title="Conditional Probability"></a>Conditional Probability</h2><p>Conditional probability is the degree of belief in a proposition given some evidence that has already been revealed. As discussed in the introduction, AI can use partial information to make educated guesses about the future. To use this information, which affects the probability that the event occurs in the future, we rely on conditional probability.</p>
<p>Conditional probability is expressed using the following notation: $P(a \mid b)$, meaning “the probability of event a occurring given that we know event b to have occurred,” or, more succinctly, “the probability of a given b.” Now we can ask questions like what is the probability of rain today given that it rained yesterday $P(rain today \mid rain yesterday)$, or what is the probability of the patient having the disease given their test results $P(disease \mid test results)$.</p>
<p>Mathematically, to compute the conditional probability of a given b, we use the following formula:</p>
<p>$$P(a \mid b) &#x3D; \frac{P(a \wedge b)}{P(b)}$$</p>
<p>To put it in words, the probability that a given b is true is equal to the probability of a and b being true, divided by the probability of b. An intuitive way of reasoning about this is the thought “we are interested in the events where both a and b are true (the numerator), but only from the worlds where we know b to be true (the denominator).” Dividing by b restricts the possible worlds to the ones where b is true. The following are algebraically equivalent forms to the formula above:</p>
<p>$$P(a \wedge b) &#x3D; P(b)P(a \mid b)$$</p>
<p>$$P(a \wedge b) &#x3D; P(a)P(b \mid a)$$</p>
<h2 id="Random-Varibles"><a href="#Random-Varibles" class="headerlink" title="Random Varibles"></a>Random Varibles</h2><p>A random variable is a variable in probability theory with a domain of possible values that it can take on. For example, to represent possible outcomes when rolling a die, we can define a random variable Roll, that can take on the values {0, 1, 2, 3, 4, 5, 6}. To represent the status of a flight, we can define a variable Flight that takes on the values {on time, delayed, canceled}.</p>
<p>Often, we are interested in the probability with which each value occurs. We represent this using a probability distribution. For example,</p>
<ul>
<li>$P(Flight &#x3D; on time) &#x3D; 0.6$</li>
<li>$P(Flight &#x3D; delayed) &#x3D; 0.3$</li>
<li>$P(Flight &#x3D; canceled) &#x3D; 0.1$<br>To interpret the probability distribution with words, this means that there is a 60% chance that the flight is on time, 30% chance that it is delayed, and 10% chance that it is canceled. Note that, as shown previously, the sum the probabilities of all possible outcomes is 1.</li>
</ul>
<p>A probability distribution can be represented more succinctly as a vector. For example, $P(Flight) &#x3D; &lt;0.6, 0.3, 0.1&gt;$. For this notation to be interpretable, the values have a set order (in our case, on time, delayed, canceled).</p>
<h3 id="Independence"><a href="#Independence" class="headerlink" title="Independence"></a>Independence</h3><p>Independence is the knowledge that the occurrence of one event does not affect the probability of the other event. For example, when rolling two dice, the result of each die is independent from the other. Rolling a 4 with the first die does not influence the value of the second die that we roll. This is opposed to dependent events, like clouds in the morning and rain in the afternoon. If it is cloudy in the morning, it is more likely that it will rain in the morning, so these events are dependent.</p>
<p>Independence can be defined mathematically: events a and b are independent if and only if the probability of a and b is equal to the probability of a times the probability of b: $P(a \wedge b) &#x3D; P(a)P(b)$.</p>
<h2 id="Bayes’-Rule"><a href="#Bayes’-Rule" class="headerlink" title="Bayes’ Rule"></a>Bayes’ Rule</h2><p>Bayes’ rule is commonly used in probability theory to compute conditional probability. In words, Bayes’ rule says that the probability of b given a is equal to the probability of a given b, times the probability of b, divided by the probability of a.</p>
<p>$$<br>P(b \mid a) &#x3D; \frac{P(b)P(a \mid b)}{P(a)}$$</p>
<p>Knowing $P(a \mid b)$, in addition to $P(a)$ and $P(b)$, allows us to calculate $P(b \mid a)$. This is helpful, because knowing the conditional probability of a visible effect given an unknown cause, P(visible effect | unknown cause), allows us to calculate the probability of the unknown cause given the visible effect, P(unknown cause | visible effect). For example, we can learn P(medical test results | disease) through medical trials, where we test people with the disease and see how often the test picks up on that. Knowing this, we can calculate P(disease | medical test results), which is valuable diagnostic information.</p>
<h2 id="Joint-Probability"><a href="#Joint-Probability" class="headerlink" title="Joint Probability"></a>Joint Probability</h2><p>Joint probability is the likelihood of multiple events all occurring.</p>
<p>We need to look at the joint probabilities of all the possible outcones of thhe 2 varibles.</p>
<table>
<thead>
<tr>
<th>C &#x3D; cloud</th>
<th>C &#x3D; $\neg$ cloud</th>
</tr>
</thead>
<tbody><tr>
<td>0.4</td>
<td>0.6</td>
</tr>
</tbody></table>
<table>
<thead>
<tr>
<th>R &#x3D; rain</th>
<th>R &#x3D; $\neg$ rain</th>
</tr>
</thead>
<tbody><tr>
<td>0.1</td>
<td>0.9</td>
</tr>
</tbody></table>
<table>
<thead>
<tr>
<th></th>
<th>R &#x3D; rain</th>
<th>R &#x3D; $\neg$ rain</th>
</tr>
</thead>
<tbody><tr>
<td><strong>C</strong> &#x3D; cloud</td>
<td>0.08</td>
<td>0.32</td>
</tr>
<tr>
<td><strong>C</strong> &#x3D; $\neg$ cloud</td>
<td>0.02</td>
<td>0.58</td>
</tr>
</tbody></table>
<blockquote>
<p>In fact, the joint probability here is not the result of calculation, but a rule of thought.</p>
</blockquote>
<p>Now we are able to know information about the co-occurrence of the events. For example, we know that the probability of a certain day having clouds in the morning and rain in the afternoon is 0.08. The probability of no clouds in the morning and no rain in the afternoon is 0.58.</p>
<p>Using joint probabilities, we can deduce conditional probability. For example, if we are interested in the probability distribution of clouds in the morning given rain in the afternoon. $P(C \mid rain) &#x3D; \frac{P(C, rain)}{P(rain)}$ (a side note: in probability, commas and $\wedge$ are used interchangeably. Thus, $P(C, rain) &#x3D; P(C \wedge rain))$. In words, we divide the joint probability of rain and clouds by the probability of rain.</p>
<p>In the last equation, it is possible to view $P(rain)$ as some constant by which $P(C, rain)$ is multiplied. Thus, we can rewrite $\frac{P(C, rain)}{P(rain)} &#x3D; \alpha P(C, rain)$, or $\alpha &lt;0.08, 0.02&gt;$. Factoring out α leaves us with the proportions of the probabilities of the possible values of C given that there is rain in the afternoon. Namely, if there is rain in the afternoon, the proportion of the probabilities of clouds in the morning and no clouds in the morning is 0.08:0.02. Note that 0.08 and 0.02 don’t sum up to 1; however, since this is the probability distribution for the random variable C, we know that they should sum up to 1. Therefore, we need to normalize the values by computing α such that $\alpha 0.08 + \alpha 0.02 &#x3D; 1$. Finally, we can say that $P(C \mid rain) &#x3D; &lt;0.8, 0.2&gt;$.</p>
<p>因为这中间需要包涵到cloud的两种情况，即晴天或者多云，两种情况的概率总和为1,没有第三种可能的情况，所以两种情况的等比扩大和应为1（也可以理解为矩阵加法），由此我们可以推导出对应特定条件的单一条件的概率。</p>
<h2 id="Probability-Rules"><a href="#Probability-Rules" class="headerlink" title="Probability Rules"></a>Probability Rules</h2><p>Negation: $P(\neg a) &#x3D; 1 - P(a)$. This stems from the fact that the sum of the probabilities of all the possible worlds is 1, and the complementary literals $a$ and $\neg a$ include all the possible worlds.</p>
<p>Inclusion-Exclusion: $P(a \vee b) &#x3D; P(a) + P(b) - P(a \wedge b)$. This can interpreted in the following way: the worlds in which a or b are true are equal to all the worlds where a is true, plus the worlds where b is true. However, in this case, some worlds are counted twice (the worlds where both a and b are true)). To get rid of this overlap, we subtract once the worlds where both a and b are true (since they were counted twice).</p>
<p>Here is an example from outside lecture that can elucidate this. Suppose I eat ice cream 80% of days and cookies 70% of days. If we’re calculating the probability that today I eat ice cream or cookies $P(ice cream \vee cookies)$ without subtracting $P(ice cream \wedge cookies)$, we erroneously end up with 0.7 + 0.8 &#x3D; 1.5. This contradicts the axiom that probability ranges between 0 and 1. To correct for counting twice the days when I ate both ice cream and cookies, we need to subtract $P(ice cream \wedge cookies)$ once.</p>
<h3 id="Marginalization"><a href="#Marginalization" class="headerlink" title="Marginalization"></a>Marginalization</h3><p>$P(a) &#x3D; P(a, b) + P(a, \neg b)$. The idea here is that $b$ and $\neg b$ are disjoint probabilities. That is, the probability of $b$ and $\neg b$ occurring at the same time is 0. We also know $b$ and $\neg b$ sum up to 1. Thus, when a happens, $b$ can either happen or not. When we take the probability of both a and b happening in addition to the probability of $a$ and $\neg b$, we end up with simply the probability of a.</p>
<p>Marginalization can be expressed for random variables the following way:</p>
<p>$$P(X &#x3D; x_{i}) &#x3D; \sum_{j}P(X &#x3D; x_{i}, Y &#x3D; y_{i})$$</p>
<p>The left side of the equation means “The probability of random variable X having the value $x_{i}$.” For example, for the variable C we mentioned earlier, the two possible values are clouds in the morning and no clouds in the morning. The right part of the equation is the idea of marginalization. $P(X &#x3D; x_{i})$ is equal to the sum of all the joint probabilities of $x_{i}$ and every single value of the random variable Y.</p>
<h3 id="Conditioning"><a href="#Conditioning" class="headerlink" title="Conditioning"></a>Conditioning</h3><p>$P(a) &#x3D; P(a \mid b)P(b) + P(a \mid \neg b)P(\neg b)$. This is $a$ similar idea to marginalization. The probability of event $a$ occurring is equal to the probability of a given $b$ times the probability of $b$, plus the probability of a given $\neg b$ time the probability of $\neg b$.</p>
<p>$$P(X &#x3D; x_{i}) &#x3D; \sum_{j}P(X &#x3D; x_{i} \mid Y &#x3D; y_{i})P(Y &#x3D; y_{i})$$</p>
<p>In this formula, the random variable X takes the value $x_{i}$ with probability that is equal to the sum of the probabilities of $x_{i}$ given each value of the random variable Y multiplied by the probability of variable Y taking that value. This makes sense if we remember that $P(a \mid b) &#x3D; \frac{P(a, b)}{P(b)}$. If we multiply this expression by $P(b)$, we end up with $P(a, b)$, and from here we do the same as we did with marginalization.</p>
<h2 id="Beyesian-Networks"><a href="#Beyesian-Networks" class="headerlink" title="Beyesian Networks"></a>Beyesian Networks</h2><p>A Bayesian network is a data structure that represents the dependencies among random variables. Bayesian networks have the following properties:</p>
<p>They are directed graphs.</p>
<ul>
<li>Each node on the graph represent a random variable.</li>
<li>An arrow from X to Y represents that X is a parent of Y. That is, the probability distribution of Y depends on the value of X.</li>
<li>Each node X has probability distribution $P(X \mid Parents(X))$.</li>
</ul>
<p>In other words, if we want to find the probability of events under specific conditions, in a complex Bayesian network, we only need to consider the previous conditions of the current situation and go back to the initial conditions. The conditional probability under the current conditions can be used as the result of the preconditions and go back to the initial conclusion.</p>
<p>We make A hypothesis that if A is the most initial condition, A indicates B, A also indicates C, B indicates C, and C indicates D. each condition contains three seeds. If we want to find the probability of a specific case in condition D under three different specific conditions of A, B, C. As shown in the following table:</p>
<table>
<thead>
<tr>
<th>A</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody><tr>
<td>$\downarrow$</td>
<td>$\searrow$</td>
<td></td>
</tr>
<tr>
<td><strong>B</strong></td>
<td>$\to$</td>
<td><strong>C</strong></td>
</tr>
<tr>
<td></td>
<td></td>
<td>$\downarrow$</td>
</tr>
<tr>
<td></td>
<td></td>
<td><strong>D</strong></td>
</tr>
</tbody></table>
<p>And we assume that each of the condtions contains 3 sub cases $\alpha , \beta  and  \gamma$.</p>
<p>So now, if we want to calculate the probability of $\alpha$ in condition D, when C is in $\beta$, B is in $\gamma$ and A is also in $\gamma$. We need to combine all the circumstances to compute. As the answer follows:</p>
<p>$$P（A_{\gamma}, B_{\gamma}, C_{\beta}, D_{\alpha}） &#x3D; P(D_{\alpha} \mid C_{\beta})P(C_{\beta} \mid A_{\gamma}, B_{\gamma})P(B_{\gamma} \mid A_{\gamma})P(A_{\gamma})$$</p>
<h3 id="Inference"><a href="#Inference" class="headerlink" title="Inference"></a>Inference</h3><p>We could definitively conclude new information based on the information that we already had. We can also infer new information based on probabilities. While this does not allow us to know new information for certain, it allows us to figure out the probability distributions for some values. Inference has multiple properties.</p>
<ul>
<li>Query <strong>X</strong>: the variable for which we want to compute the probability distribution.</li>
<li>Evidence variables <strong>E</strong>: one or more variables that have been observed for event e. For example, we might have observed that there is light rain, and this observation helps us compute the probability that the train is delayed.</li>
<li>Hidden variables <strong>Y</strong>: variables that aren’t the query and also haven’t been observed. For example, standing at the train station, we can observe whether there is rain, but we can’t know if there is maintenance on the track further down the road. Thus, Maintenance would be a hidden variable in this situation.</li>
<li>The <strong>goal</strong>: calculate $P(X \mid e)$. For example, compute the probability distribution of the Train variable (the query) based on the evidence e that we know there is light rain.</li>
</ul>
<h3 id="Inference-by-Enumeration"><a href="#Inference-by-Enumeration" class="headerlink" title="Inference by Enumeration"></a>Inference by Enumeration</h3><p>Inference by enumeration is a process of finding the probability distribution of variable X given observed evidence e and some hidden variables Y.</p>
<p>$$P(X \mid e) &#x3D; \alpha P(X, e) &#x3D; \alpha \sum_{y} P(X, e, y)$$</p>
<p>In this equation, $X$ stand for the query variable, $e $for the observed evidence, $y$ for all the values of the hidden variables, and $\alpha$ normalizes the result such that we end up with probabilities that add up to 1. To explain the equation in words, it is saying that the probability distribution of $X$ given $e$ is equal to a normalized probability distribution of $X$ and $e$. To get to this distribution, we sum the normalized probability of $X$, $e$, and $y$, where $y$ takes each time a different value of the hidden variables $Y$.</p>
<h2 id="Sampling"><a href="#Sampling" class="headerlink" title="Sampling"></a>Sampling</h2><p>Sampling is one technique of approximate inference. In sampling, each variable is sampled for a value according to its probability distribution. We will start with an example from outside lecture, and then cover the example from lecture.</p>
<p>Because in most cases, we don’t need to predict very accurate things, just know the probability estimation of things, which will avoid a lot of complex and cumbersome calculations. The core view of Bayes is that it is different from observing the distribution of samples under specific parameters, but to make the samples full of randomness and analyze the distribution of parameters, that is, the samples are fixed, but the parameter attributes of each sample are random.</p>
<h3 id="Likelihood-Weighting-似然加权"><a href="#Likelihood-Weighting-似然加权" class="headerlink" title="Likelihood Weighting 似然加权"></a>Likelihood Weighting 似然加权</h3><ul>
<li>Start by fixing the values for evidence variables.</li>
<li>Sample the non-evidence variables using conditional probabilities in the Bayesian network.</li>
<li>Weight each sample by its likelihood: the probability of all the evidence occurring.</li>
</ul>
<p>For example, if we have the observation that the train was on time, we will start sampling as before. We sample a value of Rain given its probability distribution, then Maintenance, but when we get to Train - we always give it the observed value, in our case, on time. Then we proceed and sample Appointment based on its probability distribution given $Train &#x3D; on time$. Now that this sample exists, we weight it by the conditional probability of the observed variable given its sampled parents. That is, if we sampled Rain and got light, and then we sampled Maintenance and got yes, then we will weight this sample by $P(Train &#x3D; on time \mid light, yes)$.</p>
<h2 id="Markov-Models"><a href="#Markov-Models" class="headerlink" title="Markov Models"></a>Markov Models</h2><p>So far, we have looked at questions of probability given some information that we observed. In this kind of paradigm, the dimension of time is not represented in any way. However, many tasks do rely on the dimension of time, such as prediction. To represent the variable of time we will create a new variable, $X$, and change it based on the event of interest, such that $X_{t}$ is the current event, $X_{t+1}$ is the next event, and so on. To be able to predict events in the future, we will use Markov Models.</p>
<h3 id="The-Markov-Assumption"><a href="#The-Markov-Assumption" class="headerlink" title="The Markov Assumption"></a>The Markov Assumption</h3><ul>
<li>将随机变量作为结点，若两个随机变量相关或者不独立，则将二者连接一条边；若给定若干随机变量，则形成一个有向图，即构成一个网络。</li>
<li>如果该网络是有向无环图，则这个网络称为贝叶斯网络。</li>
<li>如果这个图退化成线性链的方式，则得到马尔可夫模型；因为每个结点都是随机变量，将其看成各个时刻(或空间)的相关变化，以随机过程的视角，则可以看成是马尔可夫过程。</li>
<li>若上述网络是无向的，则是无向图模型，又称马尔可夫随机场或者马尔可夫网络。</li>
<li>如果在给定某些条件的前提下，研究这个马尔可夫随机场，则得到条件随机场。</li>
<li>如果使用条件随机场解决标注问题，并且进一步将条件随机场中的网络拓扑变成线性的，则得到线性链条件随机场。</li>
</ul>
<p>The Markov assumption is an assumption that the current state depends on only a finite fixed number of previous states. This is important to us. Think of the task of predicting weather. In theory, we could use all the data from the past year to predict tomorrow’s weather. However, it is infeasible, both because of the computational power this would require and because there is probably no information about the conditional probability of tomorrow’s weather based on the weather 365 days ago. Using the Markov assumption, we restrict our previous states (e.g. how many previous days we are going to consider when predicting tomorrow’s weather), thereby making the task manageable. This means that we might get a more rough approximation of the probabilities of interest, but this is often good enough for our needs. Moreover, we can use a Markov model based on the information of the one last event (e.g. predicting tomorrow’s weather based on today’s weather).</p>
<h3 id="Markov-Chain"><a href="#Markov-Chain" class="headerlink" title="Markov Chain"></a>Markov Chain</h3><p>A Markov chain is a sequence of random variables where the distribution of each variable follows the Markov assumption. That is, each event in the chain occurs based on the probability of the event before it.</p>
<p>To start constructing a Markov chain, we need a transition model that will specify the the probability distributions of the next event based on the possible values of the current event.</p>
<p>每个状态的转移只依赖于之前的n个状态，这个过程被称为1个n阶的模型，其中n是影响转移状态的数目。最简单的马尔可夫过程就是一阶过程，每一个状态的转移只依赖于其之前的那一个状态，这个也叫作马尔可夫性质。</p>
<p>$$P(X_{n+1} \mid X_{1} &#x3D; x_{1}, X_{2} &#x3D; x_{2},… , X_{n} &#x3D; x_{n} &#x3D; P(X_{n+1} &#x3D; x \mid X_{n} &#x3D; x_{n}))$$</p>
<h3 id="Hidden-Markov-Models"><a href="#Hidden-Markov-Models" class="headerlink" title="Hidden Markov Models"></a>Hidden Markov Models</h3><p>A hidden Markov model is a type of a Markov model for a system with hidden states that generate some observed event. This means that sometimes, the AI has some measurement of the world but no access to the precise state of the world. In these cases, the state of the world is called the hidden state and whatever data the AI has access to are the observations. Here are a few examples for this:</p>
<p>For a robot exploring uncharted territory, the hidden state is its position, and the observation is the data recorded by the robot’s sensors.<br>In speech recognition, the hidden state is the words that were spoken, and the observation is the audio waveforms.</p>
<p>When measuring user engagement on websites, the hidden state is how engaged the user is, and the observation is the website or app analytics.</p>
<h3 id="Sensor-Markov-Assumption"><a href="#Sensor-Markov-Assumption" class="headerlink" title="Sensor Markov Assumption"></a>Sensor Markov Assumption</h3><p>The assumption that the evidence variable depends only on the corresponding state. For example, for our models, we assume that whether people bring umbrellas to the office depends only on the weather. This is not necessarily reflective of the complete truth, because, for example, more conscientious, rain-averse people might take an umbrella with them everywhere even when it is sunny, and if we knew everyone’s personalities it would add more data to the model. However, the sensor Markov assumption ignores these data, assuming that only the hidden state affects the observation.</p>
<p>A hidden Markov model can be represented in a Markov chain with two layers. The top layer, variable X, stands for the hidden state. The bottom layer, variable E, stands for the evidence, the observations that we have.</p>
<p>Based on hidden Markov models, multiple tasks can be achieved:</p>
<ul>
<li><strong>Filtering</strong>: given observations from start until now, calculate the probability distribution for the current state. For example, given information on when people bring umbrellas form the start of time until today, we generate a probability distribution for whether it is raining today or not.</li>
<li><strong>Prediction</strong>: given observations from start until now, calculate the probability distribution for a future state.</li>
<li><strong>Smoothing</strong>: given observations from start until now, calculate the probability distribution for a past state. For example, calculating the probability of rain yesterday given that people brought umbrellas today.</li>
<li><strong>Most likely explanation</strong>: given observations from start until now, calculate most likely sequence of events.</li>
</ul>
</article><div class="post-copyright"><div class="post-copyright__title"><span class="post-copyright-info"><h>Intro to AI Chapter 3 Uncertainty</h></span></div><div class="post-copyright__type"><span class="post-copyright-info"><a href="https://jkwzs.cn/2022/02/20/Intro-to-AI-Chapter-3-Uncertainty/">https://jkwzs.cn/2022/02/20/Intro-to-AI-Chapter-3-Uncertainty/</a></span></div><div class="post-copyright-m"><div class="post-copyright-m-info"><div class="post-copyright-a" style="display: inline-block;width: 120px"><h>作者</h><div class="post-copyright-cc-info"><h>南花醉笔</h></div></div><div class="post-copyright-c" style="display: inline-block;width: 120px"><h>发布于</h><div class="post-copyright-cc-info"><h>2022-02-20</h></div></div><div class="post-copyright-u" style="display: inline-block;width: 120px"><h>更新于</h><div class="post-copyright-cc-info"><h>2022-02-23</h></div></div><div class="post-copyright-c" style="display: inline-block;width: 120px"><h>许可协议</h><div class="post-copyright-cc-info"><a class="icon" rel="noopener external nofollow noreferrer" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a><a rel="noopener external nofollow noreferrer" target="_blank" title="CC BY 4.0" href="https://creativecommons.org/licenses/by/4.0/deed.zh">CC BY 4.0</a></div></div></div></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Artificial-Intelligence/">Artificial Intelligence</a><a class="post-meta__tags" href="/tags/%E7%AC%94%E8%AE%B0/">笔记</a></div><div class="post_share"><div class="social-share" data-image="/img/img_4.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i> Donate</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="/img/wechatpay.png" target="_blank"><img class="post-qr-code-img" src="/img/wechatpay.png" alt="微信"/></a><div class="post-qr-code-desc">微信</div></li><li class="reward-item"><a href="/img/alipay.jpg" target="_blank"><img class="post-qr-code-img" src="/img/alipay.jpg" alt="支付宝"/></a><div class="post-qr-code-desc">支付宝</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2022/02/22/Vue3%E8%83%8C%E6%99%AF%E8%AE%A4%E8%AF%86%E5%92%8C%E9%9D%A2%E8%AF%95%E7%82%B9/"><img class="prev-cover" src="/img/img_5.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">Previous Post</div><div class="prev_info">Vue3背景认识和面试点</div></div></a></div><div class="next-post pull-right"><a href="/2022/02/20/Intro-to-AI-Chapter-2-Knowledge/"><img class="next-cover" src="/img/img_3.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">Next Post</div><div class="next_info">Intro to AI Chapter 2 Knowledge</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>Related Articles</span></div><div class="relatedPosts-list"><div><a href="/2022/02/20/Intro-to-AI-Chapter-2-Knowledge/" title="Intro to AI Chapter 2 Knowledge"><img class="cover" src="/img/img_3.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-02-20</div><div class="title">Intro to AI Chapter 2 Knowledge</div></div></a></div><div><a href="/2022/02/18/Intro-to-AI-Chapter-1-Search/" title="Intro to AI Chapter 1 Search"><img class="cover" src="/img/img_2.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-02-18</div><div class="title">Intro to AI Chapter 1 Search</div></div></a></div><div><a href="/2022/03/22/JavaScript%E7%AC%94%E8%AE%B0%E5%88%9D%E9%98%B6/" title="JavaScript笔记初阶"><img class="cover" src="/img/img_12.jpeg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-03-22</div><div class="title">JavaScript笔记初阶</div></div></a></div><div><a href="/2022/02/22/Vue3%E8%83%8C%E6%99%AF%E8%AE%A4%E8%AF%86%E5%92%8C%E9%9D%A2%E8%AF%95%E7%82%B9/" title="Vue3背景认识和面试点"><img class="cover" src="/img/img_5.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-02-22</div><div class="title">Vue3背景认识和面试点</div></div></a></div><div><a href="/2022/03/07/%E5%88%86%E7%B1%BB-%E5%9B%9E%E5%BD%92-%E8%81%9A%E7%B1%BB-%E9%99%8D%E7%BB%B4/" title="分类,回归,聚类,降维"><img class="cover" src="/img/img_9.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-03-07</div><div class="title">分类,回归,聚类,降维</div></div></a></div><div><a href="/2022/03/09/%E7%BC%96%E8%AF%91%E5%8E%9F%E7%90%86%E6%A6%82%E8%BF%B0-%E8%AF%AD%E4%B9%89%E6%A6%82%E8%BF%B0/" title="编译原理概述,语义概述"><img class="cover" src="/img/img_10.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-03-09</div><div class="title">编译原理概述,语义概述</div></div></a></div></div></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> Comment</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/myself.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">南花醉笔</div><div class="author-info__description">给自己买花 陪自己长大</div></div><div class="card-info-data is-center"><div class="card-info-data-item"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">12</div></a></div><div class="card-info-data-item"><a href="/tags/"><div class="headline">Tags</div><div class="length-num">11</div></a></div><div class="card-info-data-item"><a href="/categories/"><div class="headline">Categories</div><div class="length-num">7</div></a></div></div><a id="card-info-btn" target="_blank" rel="noopener external nofollow noreferrer" href="https://github.com/Wzs01049"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/Wzs01049" rel="external nofollow noreferrer" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:jkwzs0429@gmail.com" rel="external nofollow noreferrer" target="_blank" title="Email"><i class="fas fa-envelope"></i></a><a class="social-icon" href="tencent://message/?uin=1119127845&amp;Site=admin5.com&amp;Menu=yes" rel="external nofollow noreferrer" target="_blank" title="QQ"><i class="fab fa-qq"></i></a><a class="social-icon" href="https://space.bilibili.com/22035617?spm_id_from=333.1007.0.0" rel="external nofollow noreferrer" target="_blank" title="bilibili"><i class="fas fa-file-video"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">Hey👋！ 欢迎来到南花醉笔丶の个人站！ 会在这里分享学习和日常！</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Catalog</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Chapter-3-Uncertainty"><span class="toc-number">1.</span> <span class="toc-text">Chapter 3 Uncertainty</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Probability"><span class="toc-number">1.1.</span> <span class="toc-text">Probability</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Axioms-in-Probability"><span class="toc-number">1.1.1.</span> <span class="toc-text">Axioms in Probability</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Unconditional-Probability"><span class="toc-number">1.1.2.</span> <span class="toc-text">Unconditional Probability</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Conditional-Probability"><span class="toc-number">1.2.</span> <span class="toc-text">Conditional Probability</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Random-Varibles"><span class="toc-number">1.3.</span> <span class="toc-text">Random Varibles</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Independence"><span class="toc-number">1.3.1.</span> <span class="toc-text">Independence</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Bayes%E2%80%99-Rule"><span class="toc-number">1.4.</span> <span class="toc-text">Bayes’ Rule</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Joint-Probability"><span class="toc-number">1.5.</span> <span class="toc-text">Joint Probability</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Probability-Rules"><span class="toc-number">1.6.</span> <span class="toc-text">Probability Rules</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Marginalization"><span class="toc-number">1.6.1.</span> <span class="toc-text">Marginalization</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Conditioning"><span class="toc-number">1.6.2.</span> <span class="toc-text">Conditioning</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Beyesian-Networks"><span class="toc-number">1.7.</span> <span class="toc-text">Beyesian Networks</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Inference"><span class="toc-number">1.7.1.</span> <span class="toc-text">Inference</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Inference-by-Enumeration"><span class="toc-number">1.7.2.</span> <span class="toc-text">Inference by Enumeration</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Sampling"><span class="toc-number">1.8.</span> <span class="toc-text">Sampling</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Likelihood-Weighting-%E4%BC%BC%E7%84%B6%E5%8A%A0%E6%9D%83"><span class="toc-number">1.8.1.</span> <span class="toc-text">Likelihood Weighting 似然加权</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Markov-Models"><span class="toc-number">1.9.</span> <span class="toc-text">Markov Models</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#The-Markov-Assumption"><span class="toc-number">1.9.1.</span> <span class="toc-text">The Markov Assumption</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Markov-Chain"><span class="toc-number">1.9.2.</span> <span class="toc-text">Markov Chain</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Hidden-Markov-Models"><span class="toc-number">1.9.3.</span> <span class="toc-text">Hidden Markov Models</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Sensor-Markov-Assumption"><span class="toc-number">1.9.4.</span> <span class="toc-text">Sensor Markov Assumption</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Post</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2023/10/10/readme/" title="readme"><img src="/img/bg.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="readme"/></a><div class="content"><a class="title" href="/2023/10/10/readme/" title="readme">readme</a><time datetime="2023-10-09T11:41:31.000Z" title="Created 2023-10-10 00:41:31">2023-10-10</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/03/22/JavaScript%E7%AC%94%E8%AE%B0%E5%88%9D%E9%98%B6/" title="JavaScript笔记初阶"><img src="/img/img_12.jpeg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="JavaScript笔记初阶"/></a><div class="content"><a class="title" href="/2022/03/22/JavaScript%E7%AC%94%E8%AE%B0%E5%88%9D%E9%98%B6/" title="JavaScript笔记初阶">JavaScript笔记初阶</a><time datetime="2022-03-21T13:12:34.000Z" title="Created 2022-03-22 02:12:34">2022-03-22</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/03/10/%E5%89%91%E6%8C%87Offer-Day01/" title="剑指Offer Day01"><img src="/img/img_11.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="剑指Offer Day01"/></a><div class="content"><a class="title" href="/2022/03/10/%E5%89%91%E6%8C%87Offer-Day01/" title="剑指Offer Day01">剑指Offer Day01</a><time datetime="2022-03-09T12:38:32.000Z" title="Created 2022-03-10 01:38:32">2022-03-10</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/03/09/%E7%BC%96%E8%AF%91%E5%8E%9F%E7%90%86%E6%A6%82%E8%BF%B0-%E8%AF%AD%E4%B9%89%E6%A6%82%E8%BF%B0/" title="编译原理概述,语义概述"><img src="/img/img_10.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="编译原理概述,语义概述"/></a><div class="content"><a class="title" href="/2022/03/09/%E7%BC%96%E8%AF%91%E5%8E%9F%E7%90%86%E6%A6%82%E8%BF%B0-%E8%AF%AD%E4%B9%89%E6%A6%82%E8%BF%B0/" title="编译原理概述,语义概述">编译原理概述,语义概述</a><time datetime="2022-03-08T11:31:00.000Z" title="Created 2022-03-09 00:31:00">2022-03-09</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/03/07/%E5%88%86%E7%B1%BB-%E5%9B%9E%E5%BD%92-%E8%81%9A%E7%B1%BB-%E9%99%8D%E7%BB%B4/" title="分类,回归,聚类,降维"><img src="/img/img_9.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="分类,回归,聚类,降维"/></a><div class="content"><a class="title" href="/2022/03/07/%E5%88%86%E7%B1%BB-%E5%9B%9E%E5%BD%92-%E8%81%9A%E7%B1%BB-%E9%99%8D%E7%BB%B4/" title="分类,回归,聚类,降维">分类,回归,聚类,降维</a><time datetime="2022-03-07T07:58:13.000Z" title="Created 2022-03-07 20:58:13">2022-03-07</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="footer_custom_text"><div class="copyright">©undefined - 2021 Spark Wang</div><div class = "together">南花醉笔 <i class="fa fa-heart"></i> 梦醉樱殇</div><p id="ghbdages"><a class="github-badge" target="_blank" href="https://hexo.io/" rel="external nofollow noreferrer" style="margin-inline:5px"><img src="https://cdn.jsdelivr.net/gh/thefine/thefine.github.io/badge/Frame-Hexo.svg"></a><a class="github-badge" target="_blank" href="https://butterfly.js.org/" rel="external nofollow noreferrer" style="margin-inline:5px"><img src="https://cdn.jsdelivr.net/gh/thefine/thefine.github.io/badge/Theme-Butterfly.svg"></a><a class="github-badge" target="_blank" href="https://www.jsdelivr.com/" rel="external nofollow noreferrer" style="margin-inline:5px"><img src="https://cdn.jsdelivr.net/gh/thefine/thefine.github.io/badge/CDN-jsDelivr.svg"></a><a class="github-badge" target="_blank" href="https://thefine.github.io/" rel="external nofollow noreferrer" style="margin-inline:5px"><img src="https://cdn.jsdelivr.net/gh/thefine/thefine.github.io/badge/Source-Github.svg"></a><a class="github-badge" target="_blank" href="http://creativecommons.org/licenses/by-nc-sa/4.0/" rel="external nofollow noreferrer" style="margin-inline:5px"><img src="https://cdn.jsdelivr.net/gh/thefine/thefine.github.io/badge/Copyright-BY--NC--SA.svg"></a></p></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="Switch Between Traditional Chinese And Simplified Chinese">繁</button><button id="darkmode" type="button" title="Switch Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between single-column and double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="Scroll To Comments"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="Back To Top"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">Local search</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  Loading the Database</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts" type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><script>function panguFn () {
  if (typeof pangu === 'object') pangu.autoSpacingPage()
  else {
    getScript('https://cdn.jsdelivr.net/npm/pangu/dist/browser/pangu.min.js')
      .then(() => {
        pangu.autoSpacingPage()
      })
  }
}

function panguInit () {
  if (false){
    GLOBAL_CONFIG_SITE.isPost && panguFn()
  } else {
    panguFn()
  }
}

document.addEventListener('DOMContentLoaded', panguInit)</script><script src="/js/search/local-search.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.2
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container:not\([display]\)').forEach(node => {
            const target = node.parentNode
            if (target.nodeName.toLowerCase() === 'li') {
              target.parentNode.classList.add('has-jax')
            } else {
              target.classList.add('has-jax')
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script><script>function loadValine () {
  function initValine () {
    const valine = new Valine(Object.assign({
      el: '#vcomment',
      appId: 'nzI0Ix8YrcNiToJLi4r4CQhy-gzGzoHsz',
      appKey: 'Aaa8O2yLqtjpy42Ea7KgNew7',
      avatar: 'mm',
      serverURLs: '',
      emojiMaps: "",
      path: window.location.pathname,
      visitor: false
    }, null))
  }

  if (typeof Valine === 'function') initValine() 
  else getScript('https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js').then(initValine)
}

if ('Valine' === 'Valine' || !false) {
  if (false) btf.loadComment(document.getElementById('vcomment'),loadValine)
  else setTimeout(loadValine, 0)
} else {
  function loadOtherComment () {
    loadValine()
  }
}</script></div><div class="aplayer no-destroy" data-id="7305110547" data-server="netease" data-type="playlist" data-fixed="true" data-mini="true" data-listFolded="false" data-order="random" data-preload="none" data-autoplay="false" muted></div><script defer="defer" id="fluttering_ribbon" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/canvas-fluttering-ribbon.min.js"></script><script id="click-heart" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/click-heart.min.js" async="async" mobile="false"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js"></script><script src="https://cdn.jsdelivr.net/gh/metowolf/MetingJS@1.2/dist/Meting.min.js"></script><script src="https://cdn.jsdelivr.net/npm/pjax/pjax.min.js"></script><script>let pjaxSelectors = ["title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"]

var pjax = new Pjax({
  elements: 'a:not([target="_blank"])',
  selectors: pjaxSelectors,
  cacheBust: false,
  analytics: false,
  scrollRestoration: false
})

document.addEventListener('pjax:send', function () {

  // removeEventListener scroll 
  window.tocScrollFn && window.removeEventListener('scroll', window.tocScrollFn)
  window.scrollCollect && window.removeEventListener('scroll', scrollCollect)

  typeof preloader === 'object' && preloader.initLoading()
  document.getElementById('rightside').style.cssText = "opacity: ''; transform: ''"
  
  if (window.aplayers) {
    for (let i = 0; i < window.aplayers.length; i++) {
      if (!window.aplayers[i].options.fixed) {
        window.aplayers[i].destroy()
      }
    }
  }

  typeof typed === 'object' && typed.destroy()

  //reset readmode
  const $bodyClassList = document.body.classList
  $bodyClassList.contains('read-mode') && $bodyClassList.remove('read-mode')

})

document.addEventListener('pjax:complete', function () {
  window.refreshFn()

  document.querySelectorAll('script[data-pjax]').forEach(item => {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
  })

  GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

  typeof chatBtnFn === 'function' && chatBtnFn()
  typeof panguInit === 'function' && panguInit()

  // google analytics
  typeof gtag === 'function' && gtag('config', '', {'page_path': window.location.pathname});

  // baidu analytics
  typeof _hmt === 'object' && _hmt.push(['_trackPageview',window.location.pathname]);

  typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()

  // prismjs
  typeof Prism === 'object' && Prism.highlightAll()

  typeof preloader === 'object' && preloader.endLoading()
})

document.addEventListener('pjax:error', (e) => {
  if (e.request.status === 404) {
    pjax.loadUrl('/404.html')
  }
})</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><!-- hexo injector body_end start --><script data-pjax>function electric_clock_injector_config(){
                var parent_div_git = document.getElementsByClassName('sticky_layout')[0];
                var item_html = '<div class="card-widget card-clock"><div class="card-glass"><div class="card-background"><div class="card-content"><div id="hexo_electric_clock"><img id="card-clock-loading" src="https://cdn.jsdelivr.net/gh/Zfour/Butterfly-clock/clock/images/weather/loading.gif" style="height: 120px; width: 100%;" data-ll-status="loading" class="entered loading"></div></div></div></div></div>';
                console.log('已挂载electric_clock')
                // parent_div_git.innerHTML=item_html+parent_div_git.innerHTML // 无报错，但不影响使用(支持pjax跳转)
                parent_div_git.insertAdjacentHTML("afterbegin",item_html) // 有报错，但不影响使用(支持pjax跳转)
            }if( document.getElementsByClassName('sticky_layout')[0] && (location.pathname ==='all'|| 'all' ==='all')){

            electric_clock_injector_config()
        } </script><script src="https://pv.sohu.com/cityjson?ie=utf-8"></script><script data-pjax  src="https://cdn.jsdelivr.net/gh/Zfour/hexo-electric-clock@1.0.6/clock.js"></script><!-- hexo injector body_end end --></body></html>